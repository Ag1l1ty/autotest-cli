# CLAUDE.md - Contexto del Proyecto AutoTest CLI

## Version Actual
**v0.1.0** - Release inicial

## Repositorio
- **URL:** https://github.com/Ag1l1ty/autotest-cli
- **Branch principal:** main
- **Licencia:** MIT

---

## Descripcion del Proyecto

AutoTest CLI es una herramienta de linea de comandos que automatiza el analisis y testing de proyectos de software. Detecta tecnologias, analiza codigo, genera tests con IA (Claude API) y ejecuta pruebas en multiples fases.

### Lenguajes Soportados
- Python (pytest, coverage.py, pytest-mock)
- JavaScript/TypeScript (jest, vitest, c8)
- Java (JUnit 5, JaCoCo, Mockito)
- Go (go test, go cover, testify)
- Rust (cargo test, tarpaulin, mockall)
- C# (dotnet test, coverlet, Moq)

### Fases de Ejecucion
1. **Smoke** - Compilacion, dependencias, puntos de entrada
2. **Unit** - Tests unitarios existentes + generados por IA
3. **Integration** - Tests de integracion con mocks automaticos
4. **Security** - Vulnerabilidades, secretos hardcodeados
5. **Quality** - Linting, tipos, complejidad

---

## Estructura del Proyecto

```
TestingApp/
‚îú‚îÄ‚îÄ pyproject.toml              # Configuracion del proyecto y dependencias
‚îú‚îÄ‚îÄ README.md                   # Documentacion principal
‚îú‚îÄ‚îÄ CLAUDE.md                   # Este archivo - contexto para Claude
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .autotest.yaml.example      # Ejemplo de configuracion
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ autotest/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py         # Version: 0.1.0
‚îÇ       ‚îú‚îÄ‚îÄ __main__.py         # Entry point: python -m autotest
‚îÇ       ‚îú‚îÄ‚îÄ cli.py              # Comandos Typer: scan, detect, analyze, generate, execute
‚îÇ       ‚îú‚îÄ‚îÄ config.py           # AutoTestConfig (Pydantic Settings)
‚îÇ       ‚îú‚îÄ‚îÄ constants.py        # Extensiones, mappings, defaults
‚îÇ       ‚îú‚îÄ‚îÄ exceptions.py       # Jerarquia de excepciones
‚îÇ       ‚îú‚îÄ‚îÄ models/             # Contratos Pydantic v2
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ project.py      # ProjectInfo, LanguageInfo, enums
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ analysis.py     # FunctionMetrics, AnalysisReport
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ adaptation.py   # ToolChainConfig, GeneratedTest, TestStrategy
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ execution.py    # TestResult, PhaseResult, ExecutionReport
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ report.py       # ReportData, QualitySummary
‚îÇ       ‚îú‚îÄ‚îÄ detector/           # Modulo 1: Deteccion de tecnologias
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ base.py         # BaseLanguageDetector (ABC)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ scanner.py      # ProjectScanner (orquestador)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ registry.py     # @register decorator
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ languages/      # Detectores por lenguaje
‚îÇ       ‚îú‚îÄ‚îÄ analyzer/           # Modulo 2: Analisis de codigo
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ engine.py       # AnalysisEngine (async)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ complexity.py   # Complejidad ciclomatica
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ coupling.py     # Grafo de imports
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ coverage_gap.py # Funciones sin tests
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ dead_code.py    # Codigo muerto
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ parsers/        # Parsers por lenguaje
‚îÇ       ‚îú‚îÄ‚îÄ adaptation/         # Modulo 3: Generacion de tests
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ engine.py       # AdaptationEngine
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ toolchains/     # Configuracion por lenguaje
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ai/
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ generator.py           # AITestGenerator (unit tests)
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ integration_generator.py # AIIntegrationTestGenerator
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ prompts.py             # Templates por lenguaje
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ validator.py           # Validacion de tests
‚îÇ       ‚îú‚îÄ‚îÄ executor/           # Modulo 4: Ejecucion de pruebas
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ engine.py       # ExecutionEngine
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ sandbox.py      # TestSandbox (temp dir)
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ runners/        # SubprocessRunner async
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ phases/         # Ejecutores por fase
‚îÇ       ‚îú‚îÄ‚îÄ reporter/           # Modulo 5: Reportes
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ engine.py       # ReportEngine
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ terminal.py     # Rich tables/panels
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ json_reporter.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ html_reporter.py
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ templates/      # Jinja2 templates
‚îÇ       ‚îî‚îÄ‚îÄ utils/              # Utilidades
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py
‚îÇ   ‚îú‚îÄ‚îÄ fixtures/               # Proyectos de ejemplo
‚îÇ   ‚îú‚îÄ‚îÄ unit/                   # Tests unitarios
‚îÇ   ‚îî‚îÄ‚îÄ integration/            # Tests E2E
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ architecture.md
    ‚îú‚îÄ‚îÄ configuration.md
    ‚îî‚îÄ‚îÄ plugin-guide.md
```

---

## Flujo de Datos

```
CLI: autotest scan /path/to/project --open
  ‚îÇ
  ‚ñº
[1] ProjectScanner.scan(path) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ProjectInfo
  ‚îÇ                                          (lenguajes, frameworks, archivos)
  ‚ñº
[2] AnalysisEngine.analyze(ProjectInfo) ‚îÄ‚îÄ‚îÄ‚ñ∫ AnalysisReport
  ‚îÇ                                          (complejidad, acoplamiento, gaps)
  ‚ñº
[3] AdaptationEngine.adapt(ProjectInfo, AnalysisReport)
  ‚îÇ   ‚îú‚îÄ‚îÄ Selecciona toolchain por lenguaje
  ‚îÇ   ‚îú‚îÄ‚îÄ AITestGenerator genera unit tests
  ‚îÇ   ‚îî‚îÄ‚îÄ AIIntegrationTestGenerator genera integration tests con mocks
  ‚îÇ                                        ‚ñº
  ‚îÇ                                   TestStrategy
  ‚îÇ                                   (tests generados, comandos)
  ‚ñº
[4] ExecutionEngine.execute(TestStrategy)
  ‚îÇ   ‚îú‚îÄ‚îÄ Escribe tests en proyecto (tests/, tests/integration/)
  ‚îÇ   ‚îú‚îÄ‚îÄ Crea sandbox temporal
  ‚îÇ   ‚îî‚îÄ‚îÄ Ejecuta fases: smoke ‚Üí unit ‚Üí integration ‚Üí quality
  ‚îÇ                                        ‚ñº
  ‚îÇ                                   ExecutionReport
  ‚ñº
[5] ReportEngine.report(...)
    ‚îú‚îÄ‚îÄ Terminal (Rich)
    ‚îú‚îÄ‚îÄ JSON (CI/CD)
    ‚îî‚îÄ‚îÄ HTML (interactivo con ID unico)
         ‚ñº
    {proyecto}/reports/autotest-report-AT-YYYYMMDD-XXXXXX.html
```

---

## Ajustes y Fixes Realizados

### 1. Integracion de Tests de Integracion
**Problema:** Los tests de integracion no se generaban en el pipeline completo.
**Causa:** `cli.py` tenia hardcodeado `phases="smoke,unit,quality"` sin "integration".
**Solucion:** Actualizado a `phases="smoke,unit,integration,quality"` en cli.py:50.

### 2. Generacion de Mocks Automaticos
**Implementado:** `AIIntegrationTestGenerator` detecta automaticamente:
- Conexiones a Supabase, Airtable, Firebase
- Llamadas HTTP (requests, httpx, aiohttp)
- Accesos a OneDrive, Google Drive, AWS S3
- Base de datos (SQLAlchemy, psycopg2, pymongo)

Los tests generados incluyen mocks con `unittest.mock.patch`.

### 3. Persistencia de Tests Generados
**Problema:** Tests se perdian al terminar el sandbox.
**Solucion:** `ExecutionEngine._write_tests_to_project()` guarda tests en:
- `{proyecto}/tests/` - unit tests
- `{proyecto}/tests/integration/` - integration tests

### 4. Reportes HTML con ID Unico
**Implementado:** Cada reporte tiene codigo `AT-YYYYMMDD-XXXXXX`:
- Aparece en nombre de archivo
- Aparece en header y footer del HTML
- Formato: `autotest-report-AT-20260203-A1B2C3.html`

### 5. UX de Reportes Mejorada
**Cambios:**
- Reportes se guardan en `{proyecto}/reports/`
- Output por defecto: `terminal,html`
- Flag `--open` abre HTML en navegador
- Panel final muestra rutas de reportes generados

### 6. Soporte de API Key Dual
**Config:** Acepta `ANTHROPIC_API_KEY` o `AUTOTEST_AI_API_KEY`.

### 7. PYTHONPATH para Tests Generados
**Fix:** Tests generados incluyen `sys.path.insert()` para importar modulos del proyecto.

---

## Comandos CLI

```bash
# Pipeline completo con reporte HTML
autotest scan ./proyecto --open

# Solo detectar tecnologias
autotest detect ./proyecto

# Analizar codigo
autotest analyze ./proyecto

# Generar estrategia + tests IA
autotest generate ./proyecto

# Ejecutar pruebas
autotest execute ./proyecto

# Opciones
autotest scan ./proyecto \
  --output terminal,json,html \
  --phases smoke,unit,integration,quality \
  --no-ai \
  --verbose \
  --fail-fast \
  --open
```

---

## Instalacion

```bash
# Desde GitHub
pip install git+https://github.com/Ag1l1ty/autotest-cli.git

# Desarrollo local
git clone https://github.com/Ag1l1ty/autotest-cli.git
cd autotest-cli
pip install -e ".[dev]"

# Configurar API key
export ANTHROPIC_API_KEY="sk-ant-api03-..."
```

---

## Dependencias Principales

| Paquete | Version | Uso |
|---------|---------|-----|
| typer | >=0.15 | CLI framework |
| rich | >=13.9 | Terminal formatting |
| pydantic | >=2.10 | Data models |
| pydantic-settings | >=2.6 | Config loading |
| radon | >=6.0 | Complejidad Python |
| anthropic | >=0.40 | Claude API |
| jinja2 | >=3.1 | HTML templates |
| pyyaml | >=6.0 | Config YAML |

---

## Modelo de IA

- **Modelo:** claude-sonnet-4-20250514
- **Uso:** Generacion de unit tests e integration tests
- **Tokens:** ~2000 por funcion (unit), ~3000 por modulo (integration)
- **Validacion:** Sintaxis Python, imports peligrosos bloqueados

---

## Archivos de Configuracion

### .autotest.yaml
```yaml
phases:
  - smoke
  - unit
  - integration
  - quality
output_formats:
  - terminal
  - html
ai_enabled: true
ai_model: claude-sonnet-4-20250514
complexity_threshold: 10
timeout_seconds: 300
```

### pyproject.toml
```toml
[tool.autotest]
phases = ["smoke", "unit", "integration", "quality"]
output_formats = ["terminal", "html"]
```

---

## Changelog

### v0.1.0 (2026-02-03)
- Release inicial
- Detector de 6 lenguajes
- Analizador de complejidad, acoplamiento, coverage gaps
- Generador de tests con Claude API (unit + integration)
- Ejecutor de 5 fases con sandbox
- Reportes: Terminal, JSON, HTML con ID unico
- CLI: scan, detect, analyze, generate, execute
- Publicado en GitHub: Ag1l1ty/autotest-cli

---

## Proximos Pasos Sugeridos

1. **Cache de analisis** - `.autotest_cache/` para re-ejecutar solo archivos cambiados
2. **Modo incremental** - `--changed-only` con git diff
3. **Watch mode** - `--watch` para re-ejecutar al cambiar archivos
4. **JUnit XML** - Formato nativo para GitHub Actions/Jenkins
5. **Docker sandbox** - Aislamiento completo para tests IA
6. **Estimacion de costo** - Mostrar tokens/costo antes de llamar a Claude
7. **Perfiles** - `.autotest.yaml` con perfiles (ci, thorough, security-only)

---

## Diagn√≥stico Actual (2026-02-03)

### Estado: PROTOTIPO T√âCNICO - NO LISTO PARA PRODUCCI√ìN

La app funciona t√©cnicamente pero **no resuelve el problema real del desarrollador**.

### Problemas Identificados

| Problema | Impacto | Ejemplo |
|----------|---------|---------|
| **Tests IA con errores** | Alto | Genera `TranscriptionProcessor` cuando la clase es `TranscriptProcessor` |
| **Errores "UNKNOWN"** | Alto | 12/18 tests muestran "revisar logs" - no es √∫til |
| **No encuentra bugs reales** | Cr√≠tico | Tests fallan por errores de la IA, no por bugs del c√≥digo |
| **Linting opaco** | Medio | Solo dice "ruff failed", no qu√© reglas ni d√≥nde |
| **Mucho ruido** | Alto | 18 errores sin prioridad, dif√≠cil saber qu√© importa |

### Lo que el desarrollador necesita vs lo que la app da

```
NECESITA: "Tu funci√≥n X tiene un bug en l√≠nea 45"
DA:       "18 tests fallaron"

NECESITA: "Este import est√° mal, c√°mbialo por Y"
DA:       "Error de importaci√≥n, revisa logs"

NECESITA: "Copia este c√≥digo para arreglar"
DA:       "Recomendaci√≥n gen√©rica"
```

---

## Recomendaciones de Mejora (PENDIENTES)

### 1. Validar antes de generar
Antes de que la IA genere un test que importa `TranscriptProcessor`:
- Verificar que la clase/funci√≥n existe en el c√≥digo
- Verificar el nombre exacto (case-sensitive)
- Si no existe, NO generar el test

### 2. Menos tests, mejor calidad
- Limitar a 5-10 tests de alta calidad
- Priorizar funciones cr√≠ticas (alta complejidad, sin tests)
- Validar que el test compila antes de incluirlo

### 3. Salida accionable
Cambiar de:
```
‚ùå quality:python:ruff - FAIL
   Recomendaci√≥n: Revisar logs
```

A:
```
‚ùå airtable_integration.py:80
   E501: L√≠nea muy larga (120 > 100 chars)

üí° Arreglo sugerido:
   response = create_card(
       base_id=BASE_ID,
       data=payload
   )
```

### 4. Enfocarse en UNA cosa bien
Opciones:
- **Opci√≥n A**: Solo an√°lisis de calidad (complejidad, linting, tipos) - hacerlo excelente
- **Opci√≥n B**: Solo generaci√≥n de tests - hacerlo excelente
- **Opci√≥n C**: Solo detecci√≥n de bugs - hacerlo excelente

NO seguir haciendo 5 cosas a medias.

### 5. Priorizaci√≥n clara
En lugar de 18 errores planos, mostrar:
```
üî¥ CR√çTICO (arreglar antes de deploy):
   1. SQL Injection en user_input.py:45

üü° IMPORTANTE (arreglar pronto):
   2. Funci√≥n sin manejo de errores: process_file()

üü¢ MENOR (cuando tengas tiempo):
   3. L√≠nea muy larga en utils.py:80
```

---

## Pr√≥ximos Pasos (Sesi√≥n Siguiente)

1. **Decisi√≥n**: ¬øPivotar a enfoque espec√≠fico o seguir como demo t√©cnico?
2. **Si pivotea**: Elegir UNA de las opciones (A, B, o C)
3. **Implementar**: Validaci√≥n de imports antes de generar tests
4. **Mejorar**: Parser de errores de ruff/mypy para mostrar detalles reales

---

## Notas para Claude

Cuando trabajes en este proyecto:

1. **Arquitectura modular** - Cada modulo tiene su engine orquestador
2. **Contratos Pydantic** - Todos los datos pasan por modelos validados
3. **Async first** - Engines usan async/await
4. **Registry pattern** - Detectores y fases usan decorador @register
5. **Tests generados** - Se guardan en el proyecto, no solo en sandbox
6. **API key** - Busca ANTHROPIC_API_KEY o AUTOTEST_AI_API_KEY
7. **Reportes** - Siempre en `{proyecto}/reports/` con ID unico
8. **IMPORTANTE**: La app est√° en estado de prototipo. No funciona bien para uso real. Ver secci√≥n "Diagn√≥stico Actual" arriba.
